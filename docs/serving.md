# 模型部署入门介绍

- [模型部署入门介绍](#模型部署入门介绍)
  - [什么是模型部署？](#什么是模型部署)
  - [模型部署的几种方式。](#模型部署的几种方式)
    - [本地当作一个函数去调用](#本地当作一个函数去调用)
    - [当作一个独立的服务，每次都通过网络去请求](#当作一个独立的服务每次都通过网络去请求)
  - [模型部署中的优化方式](#模型部署中的优化方式)
    - [推理优化](#推理优化)
    - [网络优化](#网络优化)
  - [Reference](#reference)

## 什么是模型部署？
模型部署是训练好的模型部署到实际的生产中的过程。

所谓的模型部署就是把训练好的固定权重模型当作应用程序一样反复的调用。

## 模型部署的几种方式。
模型部署主要有以下常见的方式：
- 当作本地的函数一样调用
- 当作独立的服务，通过网络去请求

### 本地当作一个函数去调用
```python
result = my_trained_model(input)
```

- 优点：使用简单，理解容易
- 缺点：
  - 每个使用该模型的程序都要加载模型，模型很大，会造成内存和磁盘的浪费；有些模型甚至不可能在单机加载进来。
  - 往往调用模型的进程本身需要利用进行推理，带来的延时是不可以接受的。
  
### 当作一个独立的服务，每次都通过网络去请求
```python
# http service
response = requests.post('some_model_service', json=inputs)

# rpc service
responce = rpc_client.GetMyResponce('model_name', inputs)
```

- 优点：
  - 可以极大的减少模型副本数量，减少对于内存和磁盘的占用。
  - 可以通过分布式的方法加载很大的模型。
  - 可以减少推理的延时、提高硬件的利用率。
    - 可以采用高性能的并行的硬件，如 GPU 等，进行加速推理。
    - 由于训练时按照 batch 进行训练，因此用于推理的模型自然而然的就对 batch 进来的数据可以进行高效的推理。独立的服务可以更加高效的推理同时请求的多个客户端。
- 缺点：
  - 需要额外增加一个网络服务，而且网络往往具备不稳定性。
  - 输入的样本如果很大，可能会占用相当大的网络带宽。

由于拆分成为独立服务的优势很明显，因此之后的对于模型部署的讨论都是关于如何在独立服务的模式上进行的。

## 模型部署中的优化方式
常见的模型部署优化主要在两个方向：
- 针对模型本身的 —— 推理优化
- 针对网络服务的 —— 网络优化


### 推理优化
推理优化主要在一下几个方面：
- 减少模型的参数，减少模型大小
- 简化模型部分计算，加快模型推理速度
- 融合模型部分算子，减少算子之间的内存拷贝消耗
- ...

目前有一些现成的库就在做推理优化的工作：
- TensorRT (GPU Only)
- XLA
- TVM
- MLIR
- ...

### 网络优化
作为一个网络服务本身，主要关注如下的参数：
- 吞吐
- 延时

为了增加吞吐，可以采用异步 worker 等方法。
为了减少延时，可以采用 batch 推理。
当然最无脑的方式就是水平扩容。

针对上述的问题，TensorFlow 开源了一套高性能的推理服务：[TensorFlow Serving](https://github.com/tensorflow/serving)。支持 HTTP 和 GRPC 两种方式。

## Reference
- [Optimizing TensorFlow Models for Serving
](https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf)

